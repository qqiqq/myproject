broker.id=0
listeners=PLAINTEXT://localhost:9093
host.name=localhost
port=9092

#give the directory for data persistence:
log.dirs=/kafka/server0.log

#This sets the maximum size of the message that the server can receive
message.max.bytes=1000000

#This sets the number of threads running to handle the network¡¯s request
num.network.threads=5

#This sets the number of threads spawned for IO operations
num.io.threads=5

#This sets the number of threads that will be running and doing various background jobs
background.threads=3

#This sets the queue size that holds the pending messages
#while others are being processed by the IO threads
queued.max.requests=500

#socket buffer size
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=102400
socket.request.max.bytes=104857600

#This sets the number of default partitions of a topic you create
#without explicitly giving any partition size
num.partitions=1

#This defines the maximum segment size in bytes. 
#Once a segment reaches that size, a new segment file is created. 
#A topic is stored as a bunch of segment files in a directory. 
#This can also be set on a per-topic basis. Its default value is 1 GB
log.segment.bytes=1073741824

#This sets the time period after which a new segment file is created, 
#even if it has not reached the size limit
log.roll.hours=24

#Its value can be either delete or compact. 
#If the delete option is set, 
#the log segments will be deleted periodically 
#when it reaches its time threshold or size limit. 
#If the compact option is set, 
#log compaction will be used to clean up obsolete records
log.cleanup.policy=compact

#This sets the amount of time the logs segments will be retained
log.retention.hours=168

#This sets the time interval at which the logs are
#checked for deletion to meet retention policies
log.retention.check.interval.ms= 30000

#For log compaction to be enabled, it has to be set true
log.cleaner.enable=false

#This sets the number of threads that need to be working to
#clean logs for compaction.
log.cleaner.threads=3

#This defines the interval at which the logs will check
#whether any log needs cleaning.
log.cleaner.backoff.ms=15000

#This sets the time interval at which the messages are flushed to the disk.
log.flush.interval.ms=3000

#default replication factor for the automatically created topics.
default.replication.factor=3

#This is the time period within which, 
#if the leader does not receive any fetch requests, 
#it is moved out of in-sync replicas and is treated as dead.
replica.lag.time.max.ms=3000


#This sets the maximum amount of time for the leader
#to respond to a replica¡¯s fetch request.
replica.fetch.wait.max.ms=1000

#This specifies the number of threads used to replicate the messages from the leader.
num.replica.fetchers=3

#This specifies the frequency at which 
#each replica saves its high watermark in the disk for recovery
replica.high.watermark.checkpoint.interval.ms=3000

replica.socket.receive.buffer.bytes=65536


zookeeper.connect=127.0.0.1:2181/kafka,192.168.0.32:2181/kafka

#This specifies the time within which, 
#if the heartbeat from the server is not received, it is considered dead.
zookeeper.session.timeout.ms=6000

#This specifies the time a ZooKeeper follower can be behind its leader.
zookeeper.sync.time.ms=2000

#Setting this value to true will make sure that, 
#if you fetch metadata or produce messages for a nonexistent topic, 
#it will be automatically created
auto.create.topics.enable=false

#This setting, if set to true, 
#makes sure that when a shutdown is called on the broker, 
#and if it¡¯s the leader of any topic, 
#then it will gracefully move all leaders to a different broker before it shuts down
controlled.shutdown.enable=true

#This sets the maximum number of retries 
#that the broker makes to do a controlled shutdown before doing an unclean one
controlled.shutdown.max.retries=3 

#This sets the backoff time between the controlled shutdown retries
controlled.shutdown.retry.backoff.ms=5000

#If this is set to true, 
#the broker will automatically try to balance the leadership of partitions 
#among the brokers by periodically setting the leadership 
#to the preferred replica of each partition if available
auto.leader.rebalance.enable=false


leader.imbalance.per.broker.percentage=10

leader.imbalance.check.interval.seconds=300

#This setting, if set to true, 
#allows the replicas that are not in-sync replicas (ISR) to become the leader. 
#This can lead to data loss.
unclean.leader.election.enable=false

#some setting for offset topic
offsets.topic.num.partitions=50
offsets.topic.retention.minutes=1440
offsets.retention.check.interval.ms=600000
offsets.topic.replication.factor=3
offsets.topic.segment.bytes=104857600
offsets.load.buffer.size=5242880
offsets.commit.required.acks=-1
offsets.commit.timeout.ms=5000